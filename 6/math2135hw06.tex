\documentclass[12pt,a4paper]{amsart}
\thispagestyle{empty}
\usepackage{lmodern}
\usepackage[
%scale=0.75,
margin=1in,
]{geometry}
\usepackage{versions}
\newif\ifsol\solfalse

%\soltrue % comment to hide solution

\ifsol \newenvironment{solution}{\smallskip \par\noindent\textbf{Solution:}}{\hfill\qed\smallskip} 
\else \excludeversion{solution} \fi
\ifsol \newenvironment{grading}{\par\noindent\textbf{Grading:}}{\hfill\\}
\else \excludeversion{grading} \fi

\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bo}{\mathbf{0}}
%\newcommand{\bp}{\mathbf{p}}
%\newcommand{\bq}{\mathbf{q}}
\newcommand{\bu}{\mathbf u}
\newcommand{\bv}{\mathbf v}
\newcommand{\bw}{w}
\newcommand{\bx}{x}
\newcommand{\by}{y}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}

\usepackage{mdframed}
\newcommand\sol[1]{
\medskip
\begin{mdframed}
\textbf{Ans:\\} #1
\end{mdframed}
\medskip
}


\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\Nul}{Nul}
\DeclareMathOperator{\Col}{Col}


\newcommand{\vt}[2]{\left[\begin{matrix} #1 \\ #2 \end{matrix}\right]}
\newcommand{\vd}[3]{\left[\begin{matrix} #1 \\ #2 \\ #3 \end{matrix}\right]}



\begin{document}
%\maketitle
\begin{center}
{\Large\textbf{Math 2135 - Assignment 6}}\\
\medskip
Due October 11, 2024 \\
\smallskip
Completed October 9, 2024, Maxwell Rodgers
\end{center}
\medskip
\thispagestyle{empty}



\medskip

\begin{enumerate}
\item
 Compute the inverse if possible: 
\[
 A = \left[\begin{matrix} 1 & 2 & 3 \\ 0 & -1 & 2 \end{matrix}\right], \quad
 B = \left[\begin{matrix} -3 & 2 &  4 \\ 0 & 1 & -2 \\ 1 & -3 & 4 \end{matrix}\right], \quad
 C = \left[\begin{matrix} 1 & 0 & 3 \\ 0 & 1 & 1 \\ -1 & 2 & -1 \end{matrix}\right]
%  , \quad
%  D = \left[\begin{matrix} 2 & -2 & 1  \\ 0 & 0 & 0 \\ 4 & 2 & 3 \end{matrix}\right]  
\]


\sol{
  \begin{enumerate}
    \item
      Impossible, not a square matrix.
    \item
      Row reducing the matrix $$ \left[\begin{matrix} -3 & 2 & 4 & 1 & 0 & 0 \\ 0 & 1 & -2 & 0 & 1 & 0 \\ 1 & -3 & 4 & 0 & 0 & 1 \end{matrix}\right]$$
      we get the matrix $$\left[\begin{matrix} 1 & 0 & 0 & 1 & 10 & 4 \\ 0 & 1 & 0 & 1 & 8 & 3 \\ 0 & 0 & 1 & \frac{1}{2} & \frac{7}{2} & \frac{3}{2} \end{matrix}\right]$$
      and thus the inverse is $$\left[\begin{array}{rrr}
1 & 10 & 4 \\
1 & 8 & 3 \\
\frac{1}{2} & \frac{7}{2} & \frac{3}{2}
\end{array}\right]$$
    \item
      Row reducing the matrix $$ \left[\begin{array}{rrrrrr}
1 & 0 & 3 & 1 & 0 & 0 \\
0 & 1 & 1 & 0 & 1 & 0 \\
-1 & 2 & -1 & 0 & 0 & 1
\end{array}\right]$$
      we get the matrix $$ \left[\begin{array}{rrrrrr}
1 & 0 & 3 & 0 & 2 & -1 \\
0 & 1 & 1 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 & -2 & 1
\end{array}\right]$$
      Since the left side does not form the identity matrix, there is no inverse.
  \end{enumerate}
}

\item    
 Let $A,B\in \R^{n\times n}$ be invertible. Show $(A\cdot B)^{-1} = B^{-1}\cdot A^{-1}$.

\sol{
Since by definition $(A\cdot B)(A\cdot B)^{-1} = I_n$,and $A\cdot A = I_n, B\cdot B = I_n$,the inverse of $(A\cdot B)$
can be represented as $B^{-1}\cdot A^{-1}$. This can be seen, because if we take $(A\cdot B)(A\cdot B)^{-1} = A\cdot B \cdot B^{-1} \cdot A^{-1} =  A\cdot I_n \cdot A^{-1} = A \cdot A^{-1} = I_n$
}

\item
 A matrix $C\in\R^{n\times m}$ is called a {\bf left inverse} of a matrix $A\in\R^{m\times n}$ if $CA=I_n$
 (the $n\times n$ identity matrix).

\begin{enumerate}
\item    
 Show that if $A$ has a left inverse $C$, then $Ax=b$ has a unique solution for any $b\in\R^n$. 
\item
 Give an example of a matrix $A$ that has a left inverse but is not invertible.
\end{enumerate}

\sol{
  \begin{enumerate}
    \item
      If a matrix $A$ has a left inverse and $a_n$ is the nth column of $A$, then $A^{-1}a_1 = e_1, A^{-1}a_2 = e_2, \hdots, A^{-1}a_n=e_n$.
      If any of the columns of A are linearly dependant, this would imply that one of the unit vectors was a linear combination of other unit vectors, which is impossible.
      Thus, if $A$ has a left inverse, it must have columns that are linearly independant, and thus $Ax=b$ will have a unique solution for any b. %not totaly convinced that this is sound. might have to show that it is a bijection if it is a linear map.
    \item
      The matrix $\left[\begin{array}{r}
0 \\
1
\end{array}\right]$ has the left inverse $\left[\begin{array}{rr}
2 & 1
\end{array}\right]$. The matrix is not invertible because it is not square, but $\left[\begin{array}{rr}
2 & 1
\end{array}\right]\cdot\left[\begin{array}{r}
0 \\
1
\end{array}\right] = [1] = I_1$
  \end{enumerate}
}

\item
 Prove that $A=\left[\begin{matrix} a & b \\ c & d \end{matrix}\right]$ is not invertible if $ad-bc=0$.


\noindent Hint: Show that the columns of $A$ are linearly dependent. Consider the cases $a=0$ and $a\neq 0$
 separately.

 \sol{
   Case 1: $a \neq 0$, assume that $ad-bc=0$\\
   First we row reduce the matrix:
   $$\left[\begin{matrix} a & b \\ 0 & d-b\frac{c}{a} \end{matrix}\right]$$ from the assumption, $b\frac{c}{a}=d$. This means 
   the bottom row of the row reduced matrix is all zeroes and thus the columns cannot be linearly independant.
  \medskip\\
   Case 2: $a=0$, assume that $ad-bc=0$\\
   Since $a=0$ either $c$ or $b$ has to be zero to satisfy the assumption. If $c$ is zero, the matrix has a zero column which is a linear combination of any other column, and thus the columns are not linearly independant.
   If $b$ is zero, the matrix has a zero row. Thus the columns cannot be linearly independant in either case.
 }





\item Let $A$ be an {\bf upper triangular matrix}, that is,
  \[ A = \left[\begin{matrix} a_{11} & \dots &  \dots & a_{1n} \\\
        0 & \ddots &  & \vdots \\
        \vdots & \ddots & \ddots & \vdots \\
%        0 & 0 & \dots & a_{nn} \\
        0 &  \dots & 0 & a_{nn} \end{matrix}\right] \]
  with zeros below the diagonal. Show
\begin{enumerate}
\item
 $A$ is invertible iff there are no zeros in the diagonal of $A$.
\item
 If $A^{-1}$ exists, it is an upper triangular matrix as well. 

 Hint: When row reducing $[A,I_n]$ to $[I_n,A^{-1}]$, what happens to the $n$ columns on the right? 
\end{enumerate}

\sol{
  \begin{enumerate}
    \item
      Since A is already in echelon form, if there were to be a zero in one of the diagonals, this would mean that there would not be a pivot in every column.
      This in turn would mean that there would be a free variable, which would prevent the matrix from being reduced to the identity if it were put into reduced row echelon form.
    \item
      Since the identity has zeroes below all of its ones, it is already triangular. 
      When we row reduce $[A,I_n]$, all the opperations will be adding lower rows to rows above them to create zeroes above the pivots.
      In the identity matrix, this will create numbers only above the diagonal, meaning that it will reamain an upper triangular matrix.
      Thus the inverse of $A$ must also be an upper triangular matrix.
  \end{enumerate}
}
\item
Assume that $f\colon \R^n\to \R^n,\ x\mapsto A\cdot x$, is bijective. Show that $A\in \R^{n\times n}$ is invertible.

Give a formula for the inverse function $f^{-1}$.

Hint: Use that $f$ is surjective and the Invertible Matrix Theorem.

\sol{
  By the Invertible Matrix Theorem, an $n\times n$ invertible matrix must have independant columns, and span $\R^n$. This is the same as being injective and surjective respectively. 
  This means that A has to be invertible because it is bijective.\\\smallskip

  Since the definiton of an invese is $f(f^{-1}(x))=x=f^{-1}(f(x))$, $f^{-1}\colon \R^n\to \R^n, \ x\mapsto A^{-1}\cdot x$ neatly satifies this.
}

\item
% Let $l$ be a line through the origin in $\R^2$ such that the angle between $x$-axis and $l$ is $\alpha$. You can find the
% standard matrix for the reflection at $l$ as follows:
\begin{enumerate}
\item
 What is the inverse of the rotation $R$ by angle $\alpha$ counter clockwise around the origin in $\R^2$?
 What is the standard matrix of $R^{-1}$?
\item
 What is the inverse of a reflection $S$ on a line through the origin in $\R^2$? What can you say about the standard matrix
 $B$ of $S$ and its inverse? You do not have to write down $B$ for this. 
\end{enumerate}

\sol{
  \begin{enumerate}
    \item
      The inverse of a rotation counter clockwise by angle $\alpha$ is a rotation clockwise by angle $\alpha$
      \\The standard matrix for $R^{-1}$ is $A=\left[\begin{matrix} \cos(\alpha) & \sin(\alpha) \\ -\sin(\alpha) & \cos(\alpha) \end{matrix}\right]$ (the inverse of the regular rotation matrix)
    \item
      The inverse of a reflection over a line is the same reflection over the same line. This means that the matrix $B$ is its own inverse.
  \end{enumerate}
}


\item
 True of false? Explain your answer.
\begin{enumerate}
\item If $A,B$ are square matrices with $AB=I_n$, then $A$ and $B$ are invertible.
\item If $A$ is invertible, then $A^T$ is invertible.
\item Let $A\in\R^{n\times n}$ and $b\in\R^n$ such that $Ax=b$ is inconsistent. Then $\R^n\to\R^n,\ x\mapsto Ax$ is not injective.   
\end{enumerate}

\sol{
  \begin{enumerate}
    \item \textbf{True}\\
      Since the matrices are both square, they must be each other's inverses (by the invertible matrix theorem), and thus are both invertible.
    \item \textbf{True}\\
      If A is invertible, by the properties of transpose matrices we have that since $A\cdot A^{-1}=I_n$ and $I_n^T=I_n$, then $(A\cdot A^{-1})^T=(A^{-1})^T\cdot A^T=I_n$, which means that $(A^{-1})^T$ is the inverse of $A^T$
    \item \textbf{True}
      Being inconsistant means that there will be a zero row if the matrix is row reduced, which means that the matrix will have a free variable and thus not be injective (or surjective).
  \end{enumerate}
}

\end{enumerate}
\end{document}



